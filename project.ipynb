{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up to insert the collected data to the backend\n",
    "\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.future import select\n",
    "from sqlalchemy import text\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi import status\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SubtitleData(BaseModel):\n",
    "    sub: str\n",
    "    start: float\n",
    "    end: float\n",
    "\n",
    "class ImageData(BaseModel):\n",
    "    start: float\n",
    "    end: float\n",
    "    images: List[bytes]  # base64-encoded or binary content\n",
    "\n",
    "async def insert_data_async(subtitles: List[SubtitleData], images_data: List[ImageData], session: AsyncSession):\n",
    "    try:\n",
    "        # Insert Subtitles\n",
    "        for subtitle in subtitles:\n",
    "            stmt = text(\n",
    "                \"INSERT INTO subtitles (sub, start, end) VALUES (:sub, :start, :end)\"\n",
    "            )\n",
    "            await session.execute(stmt, {'sub': subtitle.sub, 'start': subtitle.start, 'end': subtitle.end})\n",
    "        \n",
    "        # Insert Images with binary data\n",
    "        for image_data in images_data:\n",
    "            for image in image_data.images:\n",
    "                stmt = text(\n",
    "                    \"INSERT INTO images (start, end, image) VALUES (:start, :end, :image)\"\n",
    "                )\n",
    "                await session.execute(stmt, {\n",
    "                    'start': image_data.start,\n",
    "                    'end': image_data.end,\n",
    "                    'image': image  # Insert binary data\n",
    "                })\n",
    "\n",
    "        # Commit all changes\n",
    "        await session.commit()\n",
    "        \n",
    "        return JSONResponse(status_code=status.HTTP_200_OK, content={\"detail\": \"Data inserted successfully\"})\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        await session.rollback()\n",
    "        return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content={\"detail\": \"Une erreur est survenue lors de l'insertion.\"})\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content={\"detail\": \"Une erreur inattendue est survenue.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41767236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Collection Pipeline (ETL) Milestone\n",
    "\"\"\"The subtitles for the videos where repeating themselves as you can see in this example\n",
    "    {\n",
    "      \"start\": \"00:00:02.990\",\n",
    "      \"end\": \"00:00:03.000\",\n",
    "      \"text\": \"in this video I would like to start the\"\n",
    "    },\n",
    "    {\n",
    "      \"start\": \"00:00:03.000\",\n",
    "      \"end\": \"00:00:04.870\",\n",
    "      \"text\": \"in this video I would like to start the discussion about convolutional new\"\n",
    "    },\n",
    "    so i implemented a logc to remove the duplication \n",
    "\"\"\"\n",
    "from datasets import Dataset, Video\n",
    "from pathlib import Path\n",
    "from piq import ssim\n",
    "from torchvision.transforms.functional import rgb_to_grayscale\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import textwrap\n",
    "from PIL import Image\n",
    "from bson import Binary\n",
    "from io import BytesIO\n",
    "\n",
    "#build the dataset\n",
    "folder_path = Path(\"./videos/\")\n",
    "all_file_paths = [str(p) for p in folder_path.rglob(\"*.mp4\")]\n",
    "# videos_object = Dataset.from_dict({\"video\": all_file_paths}).cast_column(\"video\", Video())\n",
    "data = []\n",
    "#images_data = []\n",
    "#time to add so the time can be continuous across all videos\n",
    "adjust_time = 0\n",
    "current_sub_text=\"\"\n",
    "#\n",
    "frame_sub_text = \"\"\n",
    "#for sutitle drawing\n",
    "# pick whatever point-size you like:\n",
    "#pt_size = 25\n",
    "\n",
    "# Pillow will resolve this name from its own bundled fonts:\n",
    "# font = ImageFont.truetype(\"DejaVuSans.ttf\", pt_size)\n",
    "\n",
    "# def is_unique(prev_tensor, curr_tensor, treshold=0.99):\n",
    "#     # Ensure both are in [0, 1] and float32fro\n",
    "#     prev = prev_tensor.unsqueeze(0).float() /255.0\n",
    "#     curr = curr_tensor.unsqueeze(0).float() / 255.0\n",
    "\n",
    "#     #convert to grayscale for faster comparison\n",
    "#     prev = rgb_to_grayscale(prev)\n",
    "#     curr = rgb_to_grayscale(curr)\n",
    "\n",
    "#     score = ssim(prev, curr, data_range=1.0)\n",
    "\n",
    "#     return score.item() < treshold\n",
    "\n",
    "#convert time to sec\n",
    "def to_seconds(t):\n",
    "    if isinstance(t, str):\n",
    "        t = datetime.strptime(t, \"%H:%M:%S.%f\").time()\n",
    "        return t.hour * 3600 + t.minute * 60 + t.second + t.microsecond / 1e6\n",
    "    return float(t)\n",
    "\n",
    "\n",
    "# def save_binary_image(binary_data: Binary, filename: str):\n",
    "#     image = Image.open(BytesIO(binary_data))\n",
    "#     image.save(f\"{filename}.png\")  # Add the appropriate extension (e.g., .png)\n",
    "\n",
    "\n",
    "# def encode_image_binary(pil_img: Image.Image) -> Binary:\n",
    "#     buffer = BytesIO()\n",
    "#     pil_img.save(buffer, format=\"PNG\")  # or \"JPEG\"\n",
    "#     return Binary(buffer.getvalue())\n",
    "\n",
    "\n",
    "#get the subtitles of each videos, clean the repeating segment\n",
    "#for index, video_object in enumerate(videos_object):\n",
    "for video_path in all_file_paths:\n",
    "    #video = video_object[\"video\"]\n",
    "    #i = 0\n",
    "    with open (f\"{video_path.replace('mp4','json')}\", \"r\") as metadata:\n",
    "        subs =json.load(metadata)[\"captions\"]\n",
    "        #print(subs)\n",
    "    #extract first frame\n",
    "   # prev  = next(video)[\"data\"]\n",
    "\n",
    "    prev_sub   = subs[0]\n",
    "    #print(video_path)\n",
    "    #base text to remove from next sub if present\n",
    "    base_sub_text = \"\"\n",
    "    images_per_sub = []\n",
    "    #for frame in video:\n",
    "    i = 1\n",
    "    while i <  len(subs):\n",
    "\n",
    "            #tensor = frame[\"data\"]\n",
    "\n",
    "        #if is_unique(prev, tensor):\n",
    "            start = to_seconds(subs[i][\"start\"]) \n",
    "            #frame_time = to_seconds(frame[\"pts\"])\n",
    "            #merge subtitles in  3 seconds segments\n",
    "            while i < len(subs)  and (to_seconds(subs[i][\"start\"]) - to_seconds(prev_sub[\"start\"])  < 3):\n",
    "\n",
    "                current_sub_text = subs[i][\"text\"]\n",
    "                #clean the subtitles\n",
    "                if base_sub_text in current_sub_text:\n",
    "                    current_sub_text = current_sub_text.replace(base_sub_text, \"\").strip(\" \")\n",
    "\n",
    "                    if current_sub_text !=  \"\" :\n",
    "                        base_sub_text = current_sub_text\n",
    "                frame_sub_text = frame_sub_text + \" \"+current_sub_text\n",
    "                i = i + 1\n",
    "                # if i < len(subs):\n",
    "                #    current_sub =  subs[i]\n",
    "            #convert tensor into image\n",
    "            #image = to_pil_image(tensor)\n",
    "            if frame_sub_text != \"\" and i < len(subs) :\n",
    "                end = to_seconds(subs[i][\"end\"]) + adjust_time\n",
    "                #save the subtitle\n",
    "                data.append({\n",
    "                    \"video\" : video_path,\n",
    "                    \"sub\": frame_sub_text,\n",
    "                    \"start\" :start ,\n",
    "                    \"end\" : end,\n",
    "                })\n",
    "                frame_sub_text = \"\"\n",
    "                prev_sub = subs[i]\n",
    "            i = i+ 1\n",
    "                #draw subtitles into image\n",
    "                # draw = ImageDraw.Draw(image)\n",
    "                # # wrap into ~40-char lines\n",
    "                # lines = textwrap.wrap(frame_sub_text, width=40)\n",
    "                # # compute line height (font height + spacing)\n",
    "                # bbox = font.getbbox(\"Ay\")\n",
    "                # line_height = (bbox[3] - bbox[1]) + 4\n",
    "                # bottom_padding = 60\n",
    "                # left_padding   = 60\n",
    "\n",
    "                # block_height = len(lines) * line_height\n",
    "                # y            = image.height - bottom_padding - block_height\n",
    "\n",
    "                # for line in lines:\n",
    "                #     draw = ImageDraw.Draw(image)\n",
    "                #     lines = textwrap.wrap(frame_sub_text, width=160)\n",
    "                #     bbox = font.getbbox(\"Ay\")\n",
    "                #     line_h = (bbox[3]-bbox[1]) + 4\n",
    "                #     y = image.height - 30 - line_h*len(lines)\n",
    "                #     for line in lines:\n",
    "                #         draw.text((30, y), line, fill=\"white\", font=font)\n",
    "                #         y += line_h\n",
    "                #end draw\n",
    "                #reset it so the it can be reused to store the next set of subtitle\n",
    "\n",
    "            # images_per_sub.append(encode_image_binary(image))\n",
    "            # if frame_sub_text != \"\" and i <= len(subs):\n",
    "            #     images_data.append({\"start\" : start, \"end\" : end,\"images\" :images_per_sub})\n",
    "            #     #save_binary_image(images_per_sub[0], \"./images\")\n",
    "            #     images_per_sub = []\n",
    "            #     frame_sub_text = \"\"\n",
    "                \n",
    "            #image.save(f\"unique_frames/{(frame_time + adjust_time):.3f}.png\")\n",
    "                #print(next_sub)\n",
    "            #print(data)\n",
    "            # prev = tensor\n",
    "#print(data)\n",
    "    #update it with the time on the video last frame\n",
    "    #adjust_time = adjust_time + to_seconds(current_sub[\"end\"])\n",
    "#print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Featurization Pipeline Milestone\n",
    "\"\"\"compute the embedding of every chunks, then store it into Qdrant vector database\n",
    "  \"\"\"\n",
    "#SEMANTIC CHUNKING\n",
    "\"\"\"uses sentence transformer to group the subtitles that are semantically close. we compute the embedding of two subtitles at a time \n",
    "and merge them if they are semantically close \"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, Distance, VectorParams\n",
    "import uuid\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Define collection\n",
    "collection_name = \"subtitle_chunks\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vector_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=vector_size,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# Semantic merging\n",
    "\n",
    "subtitles = data  \n",
    "merged_chunks = []\n",
    "\n",
    "if not subtitles:\n",
    "    raise ValueError(\"No subtitle data found!\")\n",
    "\n",
    "current = subtitles[0].copy()\n",
    "current_embedding = model.encode(current[\"sub\"], convert_to_tensor=True)\n",
    "\n",
    "for i in range(1, len(subtitles)):\n",
    "    next_sub = subtitles[i]\n",
    "    next_embedding = model.encode(next_sub[\"sub\"], convert_to_tensor=True)\n",
    "\n",
    "    similarity = util.cos_sim(current_embedding, next_embedding).item()\n",
    "\n",
    "    if similarity > 0.75:\n",
    "        # Merge with previous\n",
    "        current[\"end\"] = next_sub[\"end\"]\n",
    "        current[\"sub\"] += \" \" + next_sub[\"sub\"]\n",
    "        current_embedding = model.encode(current[\"sub\"], convert_to_tensor=True)\n",
    "    else:\n",
    "        merged_chunks.append(current)\n",
    "        current = next_sub.copy()\n",
    "        current_embedding = next_embedding\n",
    "\n",
    "merged_chunks.append(current)  # Add final chunk\n",
    "\n",
    "# Insert into Qdrant\n",
    "\n",
    "points = []\n",
    "for chunk in merged_chunks:\n",
    "    embedding = model.encode(chunk[\"sub\"]).tolist()\n",
    "    point_id = str(uuid.uuid4())\n",
    "    payload = {\n",
    "        \"video\": chunk[\"video\"],\n",
    "        \"start\": chunk[\"start\"],\n",
    "        \"end\": chunk[\"end\"],\n",
    "        \"text\": chunk[\"sub\"]\n",
    "    }\n",
    "    points.append(PointStruct(id=point_id, vector=embedding, payload=payload))\n",
    "\n",
    "client.upsert(collection_name=collection_name, points=points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Retrieval Milestone 1\n",
    "\"\"\" This is the first part of this milestone, it retrieves the subtitltes  that are closes to the questions from qdrant along \n",
    "with their time frames and the  video that they belongs to\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"subtitle_chunks\"\n",
    "\n",
    "# queries\n",
    "queries = [\n",
    "    \"Using only the videos, explain how ResNets work.\",\n",
    "    \"Using only the videos, explain the advantages of CNNs over fully connected networks.\",\n",
    "    \"Using only the videos, explain the binary cross entropy loss function.\"\n",
    "]\n",
    "\n",
    "# Function to search Qdrant for each query\n",
    "def search_query(query, top_k=3):\n",
    "    embedding = model.encode(query).tolist()\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    return results\n",
    "# loop to group results by video and merge subtitles\n",
    "query_results = []\n",
    "\n",
    "for query in queries:\n",
    "    results = search_query(query)\n",
    "\n",
    "    # Temporary dict to group items by video\n",
    "    video_groups = defaultdict(lambda: {\"start\": float(\"inf\"), \"end\": float(\"-inf\"), \"text\": []})\n",
    "\n",
    "    for result in results:\n",
    "        payload = result.payload\n",
    "        video = payload[\"video\"]\n",
    "        start = payload[\"start\"]\n",
    "        end = payload[\"end\"]\n",
    "        text = payload[\"text\"]\n",
    "\n",
    "        # Update min start and max end\n",
    "        video_groups[video][\"start\"] = min(video_groups[video][\"start\"], start)\n",
    "        video_groups[video][\"end\"] = max(video_groups[video][\"end\"], end)\n",
    "        video_groups[video][\"text\"].append(text)\n",
    "\n",
    "    # grouped data for the current query\n",
    "    query_items = []\n",
    "    for video, data in video_groups.items():\n",
    "        query_items.append({\n",
    "            \"video\": video,\n",
    "            \"start\": data[\"start\"],\n",
    "            \"end\": data[\"end\"],\n",
    "            \"text\": \" \".join(data[\"text\"])\n",
    "        })\n",
    "\n",
    "    query_results.append({\n",
    "        \"query\": query,\n",
    "        \"results\": query_items\n",
    "    })\n",
    "\n",
    "# Pretty print results\n",
    "#print.pprint(query_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieval Milestone 2\n",
    "\"\"\"This part retrieve the frames from the relevant videos that matches the time stamp of the \n",
    "subtitles, write the subtitles on those frames. Finally output a video that answers the question asked \"\"\"\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "folder_path = Path(\"./videos/\")\n",
    "\n",
    "#function that extract the relevant frames by timestamp range, write the subtitles on them\n",
    "#given also the video where the times stamp originated, the subttilte to write and the range of time\n",
    "def extract_frames_opencv(video_path, start_time, end_time, subtitle_text):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    end_frame = min(end_frame, total_frames)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.0\n",
    "    thickness = 2\n",
    "    y_position = 1000\n",
    "    text_color = (255, 255, 255)  # White\n",
    "\n",
    "    # Read the first frame to get dimensions and text width\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Failed to read from {video_path}\")\n",
    "        cap.release()\n",
    "        return []\n",
    "\n",
    "    text_size = cv2.getTextSize(subtitle_text, font, font_scale, thickness)[0]\n",
    "    text_width = text_size[0]\n",
    "    frame_width = frame.shape[1]\n",
    "    max_x = frame_width - text_width\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Reset position again\n",
    "    frame_index = start_frame\n",
    "    i = 0\n",
    "    total_frame_count = end_frame - start_frame\n",
    "\n",
    "    while frame_index < end_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Calculate dynamic x position\n",
    "        x = int(max_x * (i / (total_frame_count - 1))) if total_frame_count > 1 else 0\n",
    "\n",
    "        # Draw text\n",
    "        cv2.putText(frame, subtitle_text, (x, y_position), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        frame_index += 1\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "#to produce the video given the relevant frames\n",
    "def write_video(frames, output_path, fps=30):\n",
    "    if not frames:\n",
    "        print(f\"No frames to write for {output_path}\")\n",
    "        return\n",
    "\n",
    "    height, width = frames[0].shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_path}\")\n",
    "\n",
    "# processing loop\n",
    "for items in query_results:\n",
    "    print(items)\n",
    "    video_title = items[\"query\"]\n",
    "    frames = []\n",
    "\n",
    "    for result in items[\"results\"]:\n",
    "        video_path = f\"./{result['video']}\"\n",
    "        segment_frames = extract_frames_opencv(video_path, result[\"start\"], result[\"end\"], result[\"text\"])\n",
    "        frames.extend(segment_frames)\n",
    "\n",
    "        del segment_frames\n",
    "        gc.collect()\n",
    "\n",
    "    output_path = f\"./output_videos/{video_title.replace(' ', '_')}.mp4\"\n",
    "    write_video(frames, output_path)\n",
    "\n",
    "    del frames\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
